{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributed learning on the census dataset using neural network\n",
    "This notebook contains the code that was used to produce the results in the paper \"Weight Exchange in Distributed Learning\" by Julian Dorner, Samuel Favrichon and Arif Selcuk Ogrenci. The data can be found on this link : https://archive.ics.uci.edu/ml/datasets/Census+Income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "As we can see in the following link the problem of using the census dataset to preict the income of households as already been solved: http://www.mis.nsysu.edu.tw/db-book/DMProject2007Spring/6/project.pdf. However we'll use this as an example to compare differnt methods of exchanging weights between training neural networks to achieve the best accuracy possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "#Keras library to build neural networks\n",
    "from keras.models import Sequential, Graph\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesing:\n",
    "Steps : \n",
    "* collect the data from the files\n",
    "* remove unused features\n",
    "* convert categorical to numeric features (one hot encoding)\n",
    "* scale numeric attributes\n",
    "* randomly divide data and balance the classes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "                'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', \n",
    "                'hours-per-week', 'native-country', 'income']\n",
    "#We won't be using the following features in our analysis \n",
    "drop_columns = ['fnlwgt', 'education-num']\n",
    "#We concatenate the test and training dataset to have a bigger input dataset\n",
    "df_1 = pd.read_csv('./adult.data', na_values='?', names=column_names)\n",
    "df_2 = pd.read_csv('./adult.test', na_values='?', names=column_names)\n",
    "df = pd.concat((df_1, df_2))\n",
    "df = df[df.age != '|1x3 Cross validator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_numeric = df[['age', 'capital-gain', 'capital-loss', 'hours-per-week']]\n",
    "df_categoric = df[['workclass', 'education', 'marital-status','occupation', 'relationship', 'race', 'sex', 'native-country', 'income']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Scaling of the input numeric features\n",
    "scaler = StandardScaler().fit(df_numeric)\n",
    "df_transformed = scaler.transform(df_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#One hot encoding to convert categorical feature to multiple columns with numeric features\n",
    "#https://en.wikipedia.org/wiki/One-hot\n",
    "dummies = []\n",
    "for col in df_categoric:\n",
    "    dummies.append(pd.get_dummies(df[col]))\n",
    "df_encoded = pd.concat(dummies, axis=1)\n",
    "\n",
    "enc = OneHotEncoder(categorical_features='all', dtype='float',handle_unknown='error', n_values='auto', sparse=False)\n",
    "encoded_data = enc.fit_transform(df_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_array = np.concatenate((df_transformed, encoded_data), axis=1)\n",
    "input_dim = processed_array.shape[1] - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48842, 216)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We now have an array containing 216 numeric features normalized and almost 50K samples \n",
    "processed_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We split the data into a training and testing set We'll use 40K samples in our training and the rest for testing\n",
    "rng = np.random.RandomState(11)  # reproducible results with a fixed seed\n",
    "indices = np.arange(len(processed_array))\n",
    "rng.shuffle(indices)\n",
    "data_shuffled = processed_array[indices]\n",
    "test_size = 40000\n",
    "train, test = processed_array[:test_size,:], processed_array[test_size:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def init_model(i):\n",
    "    \"\"\"initialise a keras multilayer neural network\n",
    "    the parameter i is used to create a different set of initialization for each neurla network\"\"\"\n",
    "    model = Sequential()\n",
    "    #100 neurons in first hidden layer with sigmoid activation function\n",
    "    model.add(Dense(100, input_dim=input_dim, init='uniform', activation='sigmoid'))\n",
    "    #Dropout of half the neurons to avoid overfitting http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\n",
    "    model.add(Dropout(0.5))\n",
    "    #2nd layer with 2 neurons\n",
    "    model.add(Dense(2, init='uniform', activation='softmax'))\n",
    "    ada = Adagrad()\n",
    "    #We store the initials weight of each model\n",
    "    model.compile(loss='mean_squared_error', optimizer=ada, class_mode='categorical')\n",
    "    name = 'initial_weights_{}_census.h5'.format(i)\n",
    "    init_weights = model.save_weights(name, overwrite=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8842/8842 [==============================] - 0s     \n",
      "0.0162087071093\n",
      "8842/8842 [==============================] - 0s     \n",
      "[[2092   11]\n",
      " [   0 6739]]\n"
     ]
    }
   ],
   "source": [
    "#initialise model\n",
    "model = init_model(0)\n",
    "#Divide data between testing and training set\n",
    "X_train, y_train = train[:, :-2], train[:, :-3:-1]\n",
    "X_test, y_test = test[:, :-2], test[:, :-3:-1]\n",
    "#run the model for 150 epochs on the whole dataset\n",
    "model.fit(X_train, y_train, nb_epoch=150, shuffle=True, batch_size=1000, verbose=0, validation_split=0.1)\n",
    "print(model.evaluate(X_test, y_test, batch_size=1000))\n",
    "preds = model.predict_classes(X_test, batch_size=1000)\n",
    "print(\"Confusion matrix :\")\n",
    "print(confusion_matrix(y_test[:,1], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.arange(5)\n",
    "np.random.shuffle(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split =  8000\n",
    "validation = 4000\n",
    "X_test, y_test = test[validation:, :-2], test[validation:, :-3:-1]\n",
    "X_val, y_val = test[:validation, :-2], test[:validation, :-3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.237373633942\n",
      "[[   0 1158]\n",
      " [   0 3684]]\n",
      "0.00585100859906\n",
      "[[1156    2]\n",
      " [   0 3684]]\n",
      "0.237198954227\n",
      "[[   0 1158]\n",
      " [   0 3684]]\n",
      "0.237167034592\n",
      "[[   0 1158]\n",
      " [   0 3684]]\n",
      "0.237286793932\n",
      "[[   0 1158]\n",
      " [   0 3684]]\n"
     ]
    }
   ],
   "source": [
    "#run the model for a dataset divided in 5 subsets\n",
    "for i in a:\n",
    "    X_train_1, y_train_1 = train[i*split:(i+1)*split, :-2], train[i*split:(i+1)*split, :-3:-1]\n",
    "    model = init_model(0)\n",
    "    model.fit(X_train_1, y_train_1, nb_epoch=100, shuffle=True, batch_size=1000, verbose=0, validation_split=0.1)#, callbacks=[early_stopping])\n",
    "    #print(model.evaluate(X_test, y_test, batch_size=1000, verbose=0))\n",
    "    preds = model.predict_classes(X_test, batch_size=1000, verbose=0)\n",
    "    print(\"confusion matrix {}:\".format(i))\n",
    "    print(confusion_matrix(y_test[:,1], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_EPOCH = 100\n",
    "nb_block = 5\n",
    "block_size = 8000\n",
    "min_index = 0\n",
    "epoch = 0\n",
    "results = []\n",
    "loss_evolution = np.empty((100,5))\n",
    "\n",
    "#create a list of models\n",
    "models = []\n",
    "for i in range(nb_block):\n",
    "    models.append(init_model(i+1))\n",
    "#runs the collage testing method\n",
    "while epoch < MAX_EPOCH:\n",
    "    if epoch%10==0:\n",
    "        print(\"epoch nÂ° :{}\".format(epoch))\n",
    "\n",
    "    for i, model in enumerate(models):    \n",
    "        if min_index == 0 and epoch == 0:\n",
    "            model.load_weights('initial_weights.h5')\n",
    "        else:\n",
    "            model.load_weights('weigths_{}_census.h5'.format(min_index+1))\n",
    "        \n",
    "        X_train, y_train = train[i*block_size:(i+1)*block_size,:-2], train[i*block_size:(i+1)*block_size, :-3:-1]\n",
    "        model.fit(X_train, y_train, nb_epoch=1, shuffle=True, batch_size=1000, validation_split=0.1, verbose=0)\n",
    "        name = \"weigths_{}_census.h5\".format(i+1)\n",
    "        model.save_weights(name, overwrite=True)\n",
    "        results.append(model.evaluate(X_val, y_val, batch_size=1000, verbose=0))\n",
    "\n",
    "    #print(results)    \n",
    "    loss_evolution[i] = results\n",
    "    min_index = results.index(min(results))\n",
    "    results = []\n",
    "    epoch +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1051  107]\n",
      " [   0 3684]]\n",
      "[[1050  108]\n",
      " [   0 3684]]\n",
      "[[1049  109]\n",
      " [   0 3684]]\n",
      "[[1050  108]\n",
      " [   0 3684]]\n",
      "[[1158    0]\n",
      " [   0 3684]]\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    preds = model.predict_classes(X_test, batch_size=1000, verbose=0)\n",
    "    print(confusion_matrix(y_test[:,1], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_weights(model, indice):\n",
    "    name = \"weigths_{}_census.h5\".format(indice+1)\n",
    "    model.save_weights(name, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1, y1 = train[0:8000,:-2], train[0:8000, :-3:-1]\n",
    "X2, y2 = train[8000:16000,:-2], train[8000:16000, :-3:-1]\n",
    "X3, y3 = train[16000:24000,:-2], train[16000:24000, :-3:-1]\n",
    "X4, y4 = train[24000:32000,:-2], train[24000:32000, :-3:-1]\n",
    "X5, y5 = train[32000:,:-2], train[32000:, :-3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates the subsets for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[:,:-2], train[:, :-3:-1], test_size=0.5, random_state=42)\n",
    "X1, X2, y1, y2 = train_test_split(X_train, y_train, test_size=0.5, random_state=2)\n",
    "X3, X4, y3, y4 = train_test_split(X_test, y_test, test_size=0.5, random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_weights_2(models, Xs, ys, method=\"average\"):\n",
    "    \"\"\"finds the best set of weights for the method chosen in cross testing i.e. average or majority\"\"\"\n",
    "    loss_array =  np.empty((4,4))\n",
    "    for i, model in enumerate(models):\n",
    "        loss_array[i] = test_model_2(model, 4, Xs, ys)\n",
    "    if method==\"majority\":\n",
    "        best_indice = most_common(list(np.argmin(loss_array, axis=1)))\n",
    "    else:\n",
    "        best_indice = np.argmin(np.mean(loss_array, axis=1))\n",
    "        \n",
    "    return best_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model_2(model, nb_block, Xs, ys):\n",
    "    \"\"\"test one of the models against all the others subsets\"\"\"\n",
    "    loss_values = []\n",
    "    for i in range(nb_block):\n",
    "        loss_values.append(model.evaluate(Xs[i], ys[i], batch_size=1000, verbose=0))\n",
    "\n",
    "    return loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xs = [X1,X2,X3,X4]\n",
    "ys = [y1,y2,y3,y4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.042611388675868514, 0.043410747870802881, 0.042639494314789771, 0.042770495824515822]\n",
      "[0.041904450580477717, 0.041895999945700167, 0.041897670365869999, 0.041864950023591516]\n",
      "[0.041633573919534685, 0.041596679203212264, 0.041591840982437137, 0.041580136865377426]\n",
      "[0.041264043375849727, 0.041212228871881959, 0.041201391816139223, 0.041177860461175442]\n",
      "[0.040716387145221231, 0.040652271453291179, 0.040648943837732078, 0.040613694768399003]\n",
      "[0.040034050494432448, 0.039964877907186745, 0.039969194307923318, 0.039911045413464311]\n",
      "[0.039230609964579347, 0.03914747331291437, 0.039163721725344658, 0.03908301517367363]\n",
      "[0.03835273291915655, 0.038255922216922048, 0.038259987346827985, 0.038191731367260215]\n",
      "[0.037412532605230808, 0.037290447205305097, 0.037317358888685703, 0.03723915433511138]\n",
      "[0.036351911444216969, 0.036237919330596925, 0.036273089889436963, 0.036178752034902575]\n",
      "[0.035189119447022674, 0.035017394274473188, 0.035076319426298144, 0.03499798225238919]\n",
      "[0.033735078573226926, 0.0335714977234602, 0.0335765796713531, 0.033443860430270436]\n",
      "[0.031939229089766742, 0.031544818356633184, 0.031734425574541092, 0.031509350147098304]\n",
      "[0.02944893455132842, 0.02909886445850134, 0.029399728681892157, 0.029007033444941044]\n",
      "[0.026405200362205505, 0.026183582376688719, 0.026278156880289318, 0.02618373967707157]\n",
      "[0.023236916586756706, 0.022724391147494315, 0.022825085278600454, 0.022665625065565111]\n",
      "[0.019289399264380336, 0.019458169303834438, 0.019279346102848649, 0.019149997783824802]\n",
      "[0.016667315876111387, 0.015764684043824674, 0.016488978173583746, 0.015671432670205831]\n",
      "[0.013209451455622911, 0.012960520014166832, 0.012733942642807961, 0.012745636701583862]\n",
      "[0.010681923804804682, 0.010322457505390048, 0.010455568972975015, 0.010556367645040154]\n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCH = 200\n",
    "\n",
    "min_index = 0\n",
    "epoch = 0\n",
    "results = []\n",
    "models = []\n",
    "\n",
    "for i in range(4):\n",
    "    models.append(init_model(i+1))\n",
    "#runs the majority method crosstesting  \n",
    "while epoch < MAX_EPOCH:\n",
    "    for i, model in enumerate(models):\n",
    "        \n",
    "        if epoch != 0:\n",
    "            model.load_weights('weigths_{}_census.h5'.format(min_index+1))\n",
    "        model.fit(Xs[i], ys[i], nb_epoch=1, shuffle=True, batch_size=1000, validation_split=0.1, verbose=0)\n",
    "        save_weights(model, i)\n",
    "        results.append(model.evaluate(X_test, y_test, batch_size=1000, verbose=0))\n",
    "        \n",
    "    if epoch%10 == 0:\n",
    "        print(results)\n",
    "    min_index = best_weights_2(models, Xs, ys, \"majority\")\n",
    "    #print(\"min {}\".format(min_index))\n",
    "    results = []\n",
    "    epoch +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1779  324]\n",
      " [   0 6739]]\n",
      "[[1844  259]\n",
      " [   0 6739]]\n",
      "[[1870  233]\n",
      " [   0 6739]]\n",
      "[[1856  247]\n",
      " [   0 6739]]\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    preds = model.predict_classes(test[:,:-2], batch_size=1000, verbose=0)\n",
    "    print(confusion_matrix(test[:, -2], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20750543475151062, 0.21002118289470673, 0.20901274681091309, 0.20803317427635193]\n",
      "[0.21854540333151817, 0.2182774543762207, 0.21846534684300423, 0.21805630624294281]\n",
      "[0.21552538871765137, 0.21520137041807175, 0.21514877304434776, 0.21484673395752907]\n",
      "[0.21012190729379654, 0.21021044254302979, 0.21033564954996109, 0.20919886603951454]\n",
      "[0.20379667729139328, 0.20408762246370316, 0.20423764735460281, 0.20287972316145897]\n",
      "[0.19699989631772041, 0.1965523436665535, 0.19751847162842751, 0.19602303951978683]\n",
      "[0.18924149498343468, 0.18962305411696434, 0.1895323172211647, 0.18762225285172462]\n",
      "[0.18136759847402573, 0.18113085255026817, 0.18187719210982323, 0.17867915332317352]\n",
      "[0.17163272947072983, 0.16931698471307755, 0.16978064551949501, 0.16909613460302353]\n",
      "[0.16127610951662064, 0.15921389311552048, 0.15864903852343559, 0.15342552214860916]\n",
      "[0.14095119386911392, 0.13481224328279495, 0.14046195894479752, 0.13878185302019119]\n",
      "[0.1194979902356863, 0.11122330464422703, 0.11752458289265633, 0.11324632167816162]\n",
      "[0.087546810507774353, 0.088401220738887787, 0.090949440374970436, 0.084785524755716324]\n",
      "[0.069734591990709305, 0.068812061101198196, 0.062361832708120346, 0.062733317725360394]\n",
      "[0.045786550268530846, 0.041955114342272282, 0.042601470835506916, 0.04224991612136364]\n",
      "[0.034571250900626183, 0.032291942276060581, 0.030469977762550116, 0.030076761730015278]\n",
      "[0.01925119711086154, 0.022658760659396648, 0.020972205325961113, 0.02274863887578249]\n",
      "[0.016463017091155052, 0.016832624096423388, 0.018106226343661547, 0.015702226897701621]\n",
      "[0.011279544327408075, 0.011139839654788375, 0.01098159491084516, 0.011089969892054796]\n",
      "[0.0083935055881738663, 0.0073307204293087125, 0.0089155607856810093, 0.009155989158898592]\n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCH = 200\n",
    "\n",
    "min_index = 0\n",
    "epoch = 0\n",
    "results = []\n",
    "models = []\n",
    "\n",
    "for i in range(4):\n",
    "    models.append(init_model(i+1))\n",
    "#collage testing example\n",
    "while epoch < MAX_EPOCH:\n",
    "    for i, model in enumerate(models):\n",
    "        \n",
    "        if epoch != 0:\n",
    "            model.load_weights('weigths_{}_census.h5'.format(min_index+1))\n",
    "        model.fit(Xs[i], ys[i], nb_epoch=1, shuffle=True, batch_size=1000, validation_split=0.1, verbose=0)\n",
    "        save_weights(model, i)\n",
    "        results.append(model.evaluate(X_val, y_val, batch_size=1000, verbose=0))\n",
    "        \n",
    "    if epoch%10 == 0:\n",
    "        print(results)\n",
    "        \n",
    "    min_index = results.index(min(results))\n",
    "    results = []\n",
    "    epoch +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2102    1]\n",
      " [   0 6739]]\n",
      "[[2102    1]\n",
      " [   0 6739]]\n",
      "[[2102    1]\n",
      " [   0 6739]]\n",
      "[[2102    1]\n",
      " [   0 6739]]\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    preds = model.predict_classes(test[:,:-2], batch_size=1000, verbose=0)\n",
    "    print(confusion_matrix(test[:, -2], preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train each of the models separately to determine our ground performance :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_EPOCH = 200\n",
    "nb_block = 4\n",
    "block_size = 8000\n",
    "min_index = 0\n",
    "epoch = 0\n",
    "results = []\n",
    "loss_evolution = np.empty((100,4))\n",
    "\n",
    "models = []\n",
    "for i in range(nb_block):\n",
    "    models.append(init_model(i+1))\n",
    "\n",
    "while epoch < MAX_EPOCH:\n",
    "    for i, model in enumerate(models):    \n",
    "        model.fit(Xs[i], ys[i], nb_epoch=1, shuffle=True, batch_size=1000, validation_split=0.1, verbose=0)\n",
    "        results.append(model.evaluate(X_val, y_val, batch_size=1000, verbose=0))\n",
    "\n",
    "    loss_evolution[i] = results\n",
    "    min_index = results.index(min(results))\n",
    "    results = []\n",
    "    epoch +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1854  249]\n",
      " [   0 6739]]\n",
      "[[2035   68]\n",
      " [   0 6739]]\n",
      "[[1854  249]\n",
      " [   0 6739]]\n",
      "[[2068   35]\n",
      " [   0 6739]]\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    preds = model.predict_classes(test[:,:-2], batch_size=1000, verbose=0)\n",
    "    print(confusion_matrix(test[:, -2], preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebok we explore the different ways of sharing weights to increase accuracy of prediction of the census dataset. The methods compared are crosstesting and collage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
